{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuned Q&A System Using SQuAD Dataset for Enhanced Answer Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index:\n",
    "0. **Introduction**  \n",
    "\n",
    "1. **Library Imports**  \n",
    "\n",
    "2. **Load Model and dataset**  \n",
    "\n",
    "3. **Exploratory Data Analysis (EDA)**  \n",
    "\n",
    "4. **Data Preprocessing**  \n",
    "\n",
    "5. **Model Training**  \n",
    "\n",
    "6. **Model Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction\n",
    "### Overview\n",
    "This is a Question & Answer (Q&A) system. We have fine-tuned the DistilBERT model locally using the SQuAD dataset and compared the performance of the fine-tuned model with the pre-fine-tuned version of DistilBERT. The aim is to evaluate the improvements in answer extraction accuracy and model performance after fine-tuning.\n",
    "\n",
    "### What is DistilBERT Model?\n",
    "DistilBERT is a smaller, faster, and more efficient version of the BERT (Bidirectional Encoder Representations from Transformers) model. It was developed by Hugging Face with the goal of retaining most of BERT’s language understanding capabilities while reducing the model’s size and computational requirements.\n",
    "\n",
    "### What's the difference between the two models?\n",
    "1.\tdistilBertModel: This is a base DistilBERT model that I fine-tuned locally using the SQuAD dataset. Its performance depends on how well the fine-tuning process was done.\n",
    "\n",
    "2.\tpreFineTuningDistilBertModel: This is a model that has already been fine-tuned by Hugging Face using the SQuAD dataset. It is optimized for question answering tasks and ready to use without additional training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizerFast, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "from transformers import DistilBertForQuestionAnswering\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/homebrew/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "distilBertModel = DistilBertForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "squad_dataset = load_dataset(\"squad\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 87599\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 10570\n",
      "    })\n",
      "})\n",
      "{\n",
      "    \"id\": [\n",
      "        \"5733be284776f41900661182\",\n",
      "        \"5733be284776f4190066117f\"\n",
      "    ],\n",
      "    \"title\": [\n",
      "        \"University_of_Notre_Dame\",\n",
      "        \"University_of_Notre_Dame\"\n",
      "    ],\n",
      "    \"context\": [\n",
      "        \"Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \\\"Venite Ad Me Omnes\\\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\",\n",
      "        \"Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \\\"Venite Ad Me Omnes\\\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\"\n",
      "    ],\n",
      "    \"question\": [\n",
      "        \"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\",\n",
      "        \"What is in front of the Notre Dame Main Building?\"\n",
      "    ],\n",
      "    \"answers\": [\n",
      "        {\n",
      "            \"text\": [\n",
      "                \"Saint Bernadette Soubirous\"\n",
      "            ],\n",
      "            \"answer_start\": [\n",
      "                515\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"text\": [\n",
      "                \"a copper statue of Christ\"\n",
      "            ],\n",
      "            \"answer_start\": [\n",
      "                188\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(squad_dataset)\n",
    "\n",
    "formatted_output = json.dumps(squad_dataset['train'][:2], indent=4)\n",
    "print(formatted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "We created a preprocessing function that prepares both training and validation datasets for the QA system based on the SQuAD dataset. It tokenizes the context and questions, handles the positioning of answers within the context, and assigns start and end positions for the answers during training. The code uses train_dataset for training and validation_dataset for evaluation, ensuring the model is properly trained and validated on the correct input format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62850d17450f4448af96cb3f742a0668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67aec512f0c4f94877e9834e20d0b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87599 88524\n",
      "10570 10784\n"
     ]
    }
   ],
   "source": [
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "def preprocess_examples(examples, is_training=True):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    \n",
    "    # Tokenize the context and questions\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    # Handle offset_mapping and example reflection\n",
    "    offset_mapping = inputs[\"offset_mapping\"]\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    \n",
    "    if is_training:\n",
    "        answers = examples[\"answers\"]\n",
    "        start_positions = []\n",
    "        end_positions = []\n",
    "\n",
    "        for i, offset in enumerate(offset_mapping):\n",
    "            sample_idx = sample_map[i]\n",
    "            answer = answers[sample_idx]\n",
    "            start_char = answer[\"answer_start\"][0]\n",
    "            end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "            sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "            # Find the start and end of the context\n",
    "            idx = 0\n",
    "            while sequence_ids[idx] != 1:   # Skip tokens that are not in the context\n",
    "                idx += 1\n",
    "            context_start = idx\n",
    "            while sequence_ids[idx] == 1:   # Find where the context ends\n",
    "                idx += 1\n",
    "            context_end = idx - 1\n",
    "\n",
    "            # If the answer is not fully inside the context, set start and end positions to 0\n",
    "            if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "                start_positions.append(0)\n",
    "                end_positions.append(0)\n",
    "            else:\n",
    "                # Calculate the start and end token positions for the answer\n",
    "                idx = context_start\n",
    "                while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                    idx += 1\n",
    "                start_positions.append(idx - 1)\n",
    "\n",
    "                idx = context_end\n",
    "                while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                    idx -= 1\n",
    "                end_positions.append(idx + 1)\n",
    "\n",
    "        # Add start and end positions to the inputs\n",
    "        inputs[\"start_positions\"] = start_positions\n",
    "        inputs[\"end_positions\"] = end_positions\n",
    "    else:\n",
    "        # For validation, we only save the example ID and offset mapping\n",
    "        example_ids = []\n",
    "        for i in range(len(inputs[\"input_ids\"])):\n",
    "            sample_idx = sample_map[i]\n",
    "            example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "            sequence_ids = inputs.sequence_ids(i)\n",
    "            offset = offset_mapping[i]\n",
    "            inputs[\"offset_mapping\"][i] = [\n",
    "                o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "            ]\n",
    "\n",
    "        inputs[\"example_id\"] = example_ids\n",
    "\n",
    "    return inputs\n",
    "\n",
    "# Apply preprocessing to the training dataset\n",
    "train_dataset = squad_dataset[\"train\"].map(\n",
    "    lambda examples: preprocess_examples(examples, is_training=True),\n",
    "    batched=True,\n",
    "    remove_columns=squad_dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "# Apply preprocessing to the validation dataset\n",
    "validation_dataset = squad_dataset[\"validation\"].map(\n",
    "    lambda examples: preprocess_examples(examples, is_training=False),\n",
    "    batched=True,\n",
    "    remove_columns=squad_dataset[\"validation\"].column_names,\n",
    ")\n",
    "\n",
    "# Check the size of the resulting datasets\n",
    "print(len(squad_dataset[\"train\"]), len(train_dataset))\n",
    "print(len(squad_dataset[\"validation\"]), len(validation_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "### Explanation of the Training Parameter:\n",
    "\n",
    "1. **`evaluation_strategy=\"epoch\"`**:\n",
    "   - **Explanation**: This sets the evaluation frequency. In this case, evaluation will occur at the end of each training epoch.\n",
    "   - **Why it's set**: By evaluating at the end of each epoch, you can monitor the model’s performance and see how well it generalizes to the validation data after each complete pass through the training data.\n",
    "\n",
    "2. **`learning_rate=3e-5`**:\n",
    "   - **Explanation**: The learning rate controls how much to update the model's weights in response to the gradients during training. A smaller learning rate makes smaller adjustments to the weights, while a larger one makes bigger updates.\n",
    "   - **Why it's set**: A learning rate of `3e-5` is commonly used for fine-tuning pre-trained models like BERT or DistilBERT. It's small enough to ensure stable convergence but large enough to make meaningful progress in weight updates.\n",
    "\n",
    "3. **`per_device_train_batch_size=16`**:\n",
    "   - **Explanation**: This specifies how many training samples are processed in one forward and backward pass per device (e.g., GPU or CPU). A larger batch size increases memory usage but can result in faster training due to better GPU utilization.\n",
    "   - **Why it's set**: A batch size of 16 strikes a balance between memory constraints (depending on my device’s RAM/VRAM) and ensuring enough data is processed in each iteration for stable gradient updates.\n",
    "\n",
    "4. **`per_device_eval_batch_size=16`**:\n",
    "   - **Explanation**: This is the batch size used for evaluation on the validation dataset per device. Similar to training batch size but used for validation.\n",
    "   - **Why it's set**: Using the same batch size for evaluation ensures that the model processes validation data efficiently. Typically, evaluation batch sizes are kept the same or slightly larger than training batch sizes if memory allows.\n",
    "\n",
    "5. **`num_train_epochs=3`**:\n",
    "   - **Explanation**: The number of epochs defines how many times the entire training dataset is passed through the model. One epoch means that the model has seen the entire dataset once.\n",
    "   - **Why it's set**: 3 epochs are often enough for fine-tuning models like DistilBERT on smaller datasets like SQuAD. This prevents overfitting, but you can adjust this based on the dataset size and model performance.\n",
    "\n",
    "6. **`weight_decay=0.01`**:\n",
    "   - **Explanation**: Weight decay is a form of regularization that helps to prevent overfitting by adding a penalty to the loss function based on the magnitude of the model’s weights.\n",
    "   - **Why it's set**: A small weight decay (like `0.01`) is commonly used to improve generalization by discouraging overly large weights, which can lead to overfitting, especially in fine-tuning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e5ad12352174fe9adbe3567c7ff229f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16599 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5681, 'grad_norm': 18.244218826293945, 'learning_rate': 2.9096331104283392e-05, 'epoch': 0.09}\n",
      "{'loss': 1.6484, 'grad_norm': 15.80468463897705, 'learning_rate': 2.819266220856678e-05, 'epoch': 0.18}\n",
      "{'loss': 1.5156, 'grad_norm': 13.678197860717773, 'learning_rate': 2.7288993312850172e-05, 'epoch': 0.27}\n",
      "{'loss': 1.4489, 'grad_norm': 11.911229133605957, 'learning_rate': 2.6385324417133564e-05, 'epoch': 0.36}\n",
      "{'loss': 1.4034, 'grad_norm': 14.737772941589355, 'learning_rate': 2.5481655521416952e-05, 'epoch': 0.45}\n",
      "{'loss': 1.2843, 'grad_norm': 12.159255981445312, 'learning_rate': 2.4577986625700344e-05, 'epoch': 0.54}\n",
      "{'loss': 1.2776, 'grad_norm': 21.655744552612305, 'learning_rate': 2.3674317729983735e-05, 'epoch': 0.63}\n",
      "{'loss': 1.2746, 'grad_norm': 14.408231735229492, 'learning_rate': 2.2770648834267124e-05, 'epoch': 0.72}\n",
      "{'loss': 1.2048, 'grad_norm': 12.982665061950684, 'learning_rate': 2.1866979938550515e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1996, 'grad_norm': 14.543550491333008, 'learning_rate': 2.0963311042833907e-05, 'epoch': 0.9}\n",
      "{'loss': 1.1672, 'grad_norm': 13.578268051147461, 'learning_rate': 2.0059642147117295e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388b54b90d84425ebe2f1171f50d0d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/674 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 145.327, 'eval_samples_per_second': 74.205, 'eval_steps_per_second': 4.638, 'epoch': 1.0}\n",
      "{'loss': 0.9126, 'grad_norm': 9.365504264831543, 'learning_rate': 1.9155973251400687e-05, 'epoch': 1.08}\n",
      "{'loss': 0.8967, 'grad_norm': 10.738567352294922, 'learning_rate': 1.825230435568408e-05, 'epoch': 1.17}\n",
      "{'loss': 0.8716, 'grad_norm': 13.488021850585938, 'learning_rate': 1.7348635459967467e-05, 'epoch': 1.27}\n",
      "{'loss': 0.9189, 'grad_norm': 15.60020637512207, 'learning_rate': 1.644496656425086e-05, 'epoch': 1.36}\n",
      "{'loss': 0.9045, 'grad_norm': 13.920702934265137, 'learning_rate': 1.554129766853425e-05, 'epoch': 1.45}\n",
      "{'loss': 0.8807, 'grad_norm': 14.941184043884277, 'learning_rate': 1.463762877281764e-05, 'epoch': 1.54}\n",
      "{'loss': 0.8802, 'grad_norm': 12.228151321411133, 'learning_rate': 1.373395987710103e-05, 'epoch': 1.63}\n",
      "{'loss': 0.891, 'grad_norm': 15.575723648071289, 'learning_rate': 1.283029098138442e-05, 'epoch': 1.72}\n",
      "{'loss': 0.8685, 'grad_norm': 16.743860244750977, 'learning_rate': 1.1926622085667812e-05, 'epoch': 1.81}\n",
      "{'loss': 0.872, 'grad_norm': 12.756616592407227, 'learning_rate': 1.1022953189951202e-05, 'epoch': 1.9}\n",
      "{'loss': 0.8632, 'grad_norm': 16.946260452270508, 'learning_rate': 1.0119284294234592e-05, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047a8ad8028a4dcbb489646f2fcbf00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/674 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 142.6511, 'eval_samples_per_second': 75.597, 'eval_steps_per_second': 4.725, 'epoch': 2.0}\n",
      "{'loss': 0.6719, 'grad_norm': 12.660990715026855, 'learning_rate': 9.215615398517983e-06, 'epoch': 2.08}\n",
      "{'loss': 0.6537, 'grad_norm': 14.955159187316895, 'learning_rate': 8.311946502801373e-06, 'epoch': 2.17}\n",
      "{'loss': 0.6489, 'grad_norm': 16.076448440551758, 'learning_rate': 7.408277607084765e-06, 'epoch': 2.26}\n",
      "{'loss': 0.652, 'grad_norm': 13.49941635131836, 'learning_rate': 6.504608711368155e-06, 'epoch': 2.35}\n",
      "{'loss': 0.6389, 'grad_norm': 14.982282638549805, 'learning_rate': 5.6009398156515455e-06, 'epoch': 2.44}\n",
      "{'loss': 0.636, 'grad_norm': 32.58141326904297, 'learning_rate': 4.697270919934936e-06, 'epoch': 2.53}\n",
      "{'loss': 0.6277, 'grad_norm': 19.506061553955078, 'learning_rate': 3.7936020242183263e-06, 'epoch': 2.62}\n",
      "{'loss': 0.6323, 'grad_norm': 8.488372802734375, 'learning_rate': 2.889933128501717e-06, 'epoch': 2.71}\n",
      "{'loss': 0.6493, 'grad_norm': 20.377105712890625, 'learning_rate': 1.9862642327851074e-06, 'epoch': 2.8}\n",
      "{'loss': 0.6296, 'grad_norm': 13.50123119354248, 'learning_rate': 1.0825953370684982e-06, 'epoch': 2.89}\n",
      "{'loss': 0.6384, 'grad_norm': 14.529203414916992, 'learning_rate': 1.7892644135188868e-07, 'epoch': 2.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e07fd7896e450d9ed688bb14ac669e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/674 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 145.6394, 'eval_samples_per_second': 74.046, 'eval_steps_per_second': 4.628, 'epoch': 3.0}\n",
      "{'train_runtime': 11789.8511, 'train_samples_per_second': 22.525, 'train_steps_per_second': 1.408, 'train_loss': 0.9926093652080474, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16599, training_loss=0.9926093652080474, metrics={'train_runtime': 11789.8511, 'train_samples_per_second': 22.525, 'train_steps_per_second': 1.408, 'total_flos': 2.602335381127373e+16, 'train_loss': 0.9926093652080474, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",              # Directory to save the model output\n",
    "    evaluation_strategy=\"epoch\",         # Evaluate the model at the end of each epoch\n",
    "    learning_rate=3e-5,                  # Learning rate for the optimizer\n",
    "    per_device_train_batch_size=16,      # Batch size for training on each device (e.g., GPU/CPU)\n",
    "    per_device_eval_batch_size=16,       # Batch size for evaluation on each device (e.g., GPU/CPU)\n",
    "    num_train_epochs=3,                  # Number of training epochs\n",
    "    weight_decay=0.01,                   # Weight decay for regularization to prevent overfitting\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = distilBertModel,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = validation_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the model performance by a series of context-question pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: When did the Wright brothers make their first flight?\n",
      "Answer: December 17, 1903\n",
      "Confidence: 0.98\n",
      "------------------------------\n",
      "Question: Where is Mount Everest located?\n",
      "Answer: the Himalayas\n",
      "Confidence: 0.76\n",
      "------------------------------\n",
      "Question: What is the main product of photosynthesis?\n",
      "Answer: chemical energy\n",
      "Confidence: 0.07\n",
      "------------------------------\n",
      "Question: How many plays did William Shakespeare write?\n",
      "Answer: 39\n",
      "Confidence: 0.89\n",
      "------------------------------\n",
      "Question: Who invented the World Wide Web?\n",
      "Answer: Tim Berners-Lee\n",
      "Confidence: 1.00\n",
      "------------------------------\n",
      "Question: Who painted the Mona Lisa?\n",
      "Answer: Leonardo da Vinci\n",
      "Confidence: 0.95\n",
      "------------------------------\n",
      "Question: When was the United Nations founded?\n",
      "Answer: 1945\n",
      "Confidence: 0.93\n",
      "------------------------------\n",
      "Question: What is inflation?\n",
      "Answer: the rate at which the general level of prices for goods and services rises\n",
      "Confidence: 0.20\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the pipline\n",
    "question_answerer = pipeline(\"question-answering\", model=distilBertModel, tokenizer=tokenizer)\n",
    "\n",
    "# Test with multiple context-question pairs\n",
    "context_question_pairs = [\n",
    "    # History Question\n",
    "    {\n",
    "        \"context\": \"The Wright brothers, Orville and Wilbur Wright, were two American aviation pioneers generally credited with inventing, building, and flying the world's first successful motor-operated airplane. They made their first controlled, sustained flight on December 17, 1903, near Kitty Hawk, North Carolina.\",\n",
    "        \"question\": \"When did the Wright brothers make their first flight?\"\n",
    "    },\n",
    "    # Geography Question\n",
    "    {\n",
    "        \"context\": \"Mount Everest is Earth's highest mountain, located in the Himalayas. Its peak is 8,848.86 meters (29,031.7 ft) above sea level. The international border between Nepal and China runs across its summit point.\",\n",
    "        \"question\": \"Where is Mount Everest located?\"\n",
    "    },\n",
    "    # Science Question\n",
    "    {\n",
    "        \"context\": \"Photosynthesis is a process used by plants and other organisms to convert light energy, usually from the sun, into chemical energy that can be later released to fuel the organisms' activities. During photosynthesis, plants capture light energy and use it to convert carbon dioxide and water into glucose and oxygen.\",\n",
    "        \"question\": \"What is the main product of photosynthesis?\"\n",
    "    },\n",
    "    # Literature Question\n",
    "    {\n",
    "        \"context\": \"William Shakespeare was an English playwright, poet, and actor, widely regarded as the greatest writer in the English language and the world's greatest dramatist. He is often called England's national poet and the 'Bard of Avon.' His extant works, including collaborations, consist of some 39 plays, 154 sonnets, and two long narrative poems.\",\n",
    "        \"question\": \"How many plays did William Shakespeare write?\"\n",
    "    },\n",
    "    # Technology Question\n",
    "    {\n",
    "        \"context\": \"The internet is a global network of interconnected computers that communicate through standardized protocols. It was developed in the late 1960s and has since revolutionized communication, information sharing, and commerce worldwide. The World Wide Web, which was invented by Tim Berners-Lee in 1989, is a system of interlinked hypertext documents accessed via the internet.\",\n",
    "        \"question\": \"Who invented the World Wide Web?\"\n",
    "    },\n",
    "    # Art/Culture Question\n",
    "    {\n",
    "        \"context\": \"The Mona Lisa is a portrait painting by the Italian artist Leonardo da Vinci. It is considered an archetypal masterpiece of the Italian Renaissance and has been described as 'the most famous, visited, talked about, and sung about' work of art in the world. The painting is thought to depict Lisa Gherardini, the wife of a wealthy Florentine merchant, and was completed in the early 16th century.\",\n",
    "        \"question\": \"Who painted the Mona Lisa?\"\n",
    "    },\n",
    "    # Politics Question\n",
    "    {\n",
    "        \"context\": \"The United Nations (UN) is an international organization founded in 1945 after the Second World War by 51 countries, committed to maintaining international peace and security, developing friendly relations among nations, promoting social progress, better living standards, and human rights.\",\n",
    "        \"question\": \"When was the United Nations founded?\"\n",
    "    },\n",
    "    # Economics Question\n",
    "    {\n",
    "        \"context\": \"Inflation is the rate at which the general level of prices for goods and services rises, and subsequently, the purchasing power of currency falls. Central banks attempt to limit inflation, and avoid deflation, to keep the economy running smoothly. The primary measure of inflation is the inflation rate, the annual percentage change in the price index.\",\n",
    "        \"question\": \"What is inflation?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Handle every question-context pair\n",
    "for pair in context_question_pairs:\n",
    "    result = question_answerer(question=pair[\"question\"], context=pair[\"context\"])\n",
    "    print(f\"Question: {pair['question']}\")\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"Confidence: {result['score']:.2f}\")\n",
    "    print('-' * 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-fine-tuned model\n",
    "We load the model which has been fine tuned by Hugging Face using SQuAD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: When did the Wright brothers make their first flight?\n",
      "Answer: December 17, 1903\n",
      "Confidence: 0.98\n",
      "------------------------------\n",
      "Question: Where is Mount Everest located?\n",
      "Answer: the Himalayas\n",
      "Confidence: 0.67\n",
      "------------------------------\n",
      "Question: What is the main product of photosynthesis?\n",
      "Answer: light energy\n",
      "Confidence: 0.10\n",
      "------------------------------\n",
      "Question: How many plays did William Shakespeare write?\n",
      "Answer: 39\n",
      "Confidence: 0.86\n",
      "------------------------------\n",
      "Question: Who invented the World Wide Web?\n",
      "Answer: Tim Berners-Lee\n",
      "Confidence: 1.00\n",
      "------------------------------\n",
      "Question: Who painted the Mona Lisa?\n",
      "Answer: Leonardo da Vinci\n",
      "Confidence: 0.98\n",
      "------------------------------\n",
      "Question: When was the United Nations founded?\n",
      "Answer: 1945\n",
      "Confidence: 0.96\n",
      "------------------------------\n",
      "Question: What is inflation?\n",
      "Answer: the annual percentage change in the price index\n",
      "Confidence: 0.45\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the pre-fine-tuned DistilBERT model\n",
    "preFineTuningDistilBertModel = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
    "\n",
    "preFineTuning_question_answerer = pipeline(\"question-answering\", model=preFineTuningDistilBertModel, tokenizer=tokenizer)\n",
    "\n",
    "for pair in context_question_pairs:\n",
    "    result = preFineTuning_question_answerer(question=pair[\"question\"], context=pair[\"context\"])\n",
    "    print(f\"Question: {pair['question']}\")\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"Confidence: {result['score']:.2f}\")\n",
    "    print('-' * 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Performance: Fine-Tuned Model vs Hugging Face Fine-Tuned Model\n",
    "\n",
    "#### 1. **Overall Summary**:\n",
    "Both models perform similarly well in answering most questions, but there are some differences in the confidence scores and the accuracy of specific answers. Below is a more detailed comparison of how each model handled the questions:\n",
    "\n",
    "#### 2. **Performance on Individual Questions**:\n",
    "\n",
    "| **Question**                                    | **My Fine-Tuned Model Answer**                  | **My Model Confidence** | **Hugging Face Model Answer**         | **Hugging Face Confidence** | **Comparison** |\n",
    "|-------------------------------------------------|---------------------------------------------------|---------------------------|---------------------------------------|------------------------------|----------------|\n",
    "| When did the Wright brothers make their first flight? | December 17, 1903                                  | 0.98                      | December 17, 1903                     | 0.98                         | Both models perform identically. |\n",
    "| Where is Mount Everest located?                 | the Himalayas                                      | 0.76                      | the Himalayas                         | 0.67                         | Both models answered correctly; my model shows slightly higher confidence. |\n",
    "| What is the main product of photosynthesis?     | chemical energy                                    | 0.07                      | light energy                          | 0.10                         | Both models give wrong answers with low confidence; neither model performed well. |\n",
    "| How many plays did William Shakespeare write?   | 39                                                | 0.89                      | 39                                    | 0.86                         | Both models answer correctly, with my model having slightly higher confidence. |\n",
    "| Who invented the World Wide Web?                | Tim Berners-Lee                                    | 1.00                      | Tim Berners-Lee                       | 1.00                         | Both models performed perfectly. |\n",
    "| Who painted the Mona Lisa?                      | Leonardo da Vinci                                  | 0.95                      | Leonardo da Vinci                     | 0.98                         | Both models are correct, but Hugging Face’s model has higher confidence. |\n",
    "| When was the United Nations founded?            | 1945                                              | 0.93                      | 1945                                  | 0.96                         | Both models answered correctly, Hugging Face’s model has slightly higher confidence. |\n",
    "| What is inflation?                              | the rate at which the general level of prices for goods and services rises | 0.20 | the annual percentage change in the price index | 0.45                         | Hugging Face’s model gave a more concise answer and had higher confidence. |\n",
    "\n",
    "#### 3. **Key Observations**:\n",
    "\n",
    "1. **Accuracy**:\n",
    "   - Both models provided correct answers for most questions, such as **\"When did the Wright brothers make their first flight?\"**, **\"Where is Mount Everest located?\"**, **\"Who invented the World Wide Web?\"**, **\"Who painted the Mona Lisa?\"**, and **\"How many plays did William Shakespeare write?\"**.\n",
    "   - However, for the question **\"What is the main product of photosynthesis?\"**, both models provided incorrect answers with low confidence. This indicates that neither model is strong in answering this particular scientific question.\n",
    "   - On the question **\"What is inflation?\"**, the Hugging Face model provided a more accurate and concise answer, along with higher confidence.\n",
    "\n",
    "2. **Confidence**:\n",
    "   - The **confidence scores** are similar between both models in most cases, but my model shows slightly higher confidence on questions like **\"Where is Mount Everest located?\"** and **\"How many plays did William Shakespeare write?\"**.\n",
    "   - However, for some other questions like **\"What is inflation?\"** and **\"Who painted the Mona Lisa?\"**, the Hugging Face model had a higher confidence, suggesting better understanding or more optimized fine-tuning.\n",
    "\n",
    "3. **Consistency**:\n",
    "   - Both models performed almost identically on most factual questions, such as dates and names (e.g., Wright brothers' flight date, Tim Berners-Lee inventing the web).\n",
    "   - The **confidence** variance is noticeable but not large. In most cases, both models maintained fairly high confidence for correct answers and lower confidence for more uncertain or incorrect ones.\n",
    "\n",
    "4. **Edge Cases**:\n",
    "   - For more nuanced or complex questions (e.g., \"What is the main product of photosynthesis?\"), both models struggled, which suggests that more specialized fine-tuning or additional data may be necessary to improve performance on such questions.\n",
    "\n",
    "#### 4. **Which Model Performed Better?**\n",
    "- In terms of **accuracy**, both models perform equally well on most questions, providing correct answers for a majority of them.\n",
    "- In terms of **confidence**, my fine-tuned model had higher confidence in some cases, but the **Hugging Face fine-tuned model** exhibited slightly higher confidence in some other key questions, such as **\"What is inflation?\"**.\n",
    "- Overall, both models are very close in performance, with **slightly higher confidence** for the Hugging Face model in certain cases.\n",
    "\n",
    "### Conclusion:\n",
    "My fine-tuned model performed very well, comparable to the Hugging Face pre-fine-tuned model. For most questions, both models gave correct answers with high confidence. The difference in confidence scores is small, and neither model has a clear, significant advantage over the other. However, for a few specific questions, like **\"What is inflation?\"**, the Hugging Face model performed better in terms of both accuracy and confidence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
